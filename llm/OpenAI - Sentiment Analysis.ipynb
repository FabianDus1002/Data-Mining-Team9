{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e4f9d5",
   "metadata": {},
   "source": [
    "# Evaluating GPT 3.5 on sentiment analysis\n",
    "In this notebook, the GPT 3.5 model from OpenAI will be evaluated on a sentiment analysis task. The model will be applied through an API, and will be asked to categorize ratings from different exchange students into the sentiments \"positive\", \"neutral\" or \"negative\". Evaluation will be conducted using a confusion matrix, which will compare the sentiments predicted by GPT with manually labeled sentiments. Additionally, precision, recall, and F1 scores will be computed to provide a more comprehensive assessment of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc1711",
   "metadata": {},
   "source": [
    "# Data import \n",
    "In this section every labelled data from the student data set is imported. As GPT will be evaluated on its sentiment categorization, only labelled datasets are utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4256aba",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Studium_Label'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#student_data_unpr = student_data_unpr.drop(\"Unnamed: 0\", axis=1)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#student_data_unpr = student_data_unpr.drop(\"Studium_Comment\", axis=1)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#Creation of a copy and reduction to column \"Studium\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m student_data \u001b[38;5;241m=\u001b[39m student_data_unpr\n\u001b[1;32m---> 15\u001b[0m student_data \u001b[38;5;241m=\u001b[39m student_data\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudium_Label\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5112\u001b[0m     labels: IndexLabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5122\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5123\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5256\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5259\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5260\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5261\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5262\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5263\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5264\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5265\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5266\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4549\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4589\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4591\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4592\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4594\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6699\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6700\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Studium_Label'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import of data\n",
    "#student_data_unpr = pd.read_excel(\"../data/subset_studium_german.xlsx\")\n",
    "#student_data_unpr = student_data_unpr.drop(\"Unnamed: 0\", axis=1)\n",
    "#student_data_unpr = student_data_unpr.drop(\"Studium_Comment\", axis=1)\n",
    "\n",
    "#Reduction to labeled entries\n",
    "#student_data_unpr = student_data_unpr.drop(student_data_unpr.index[1020:], axis=0)\n",
    "\n",
    "#Creation of a copy and reduction to column \"Studium\"\n",
    "student_data = student_data_unpr\n",
    "student_data = student_data.drop(\"Studium_Label\", axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29b04b",
   "metadata": {},
   "source": [
    "# [1] OpenAI GPT3.5 application\n",
    "\n",
    "Firstly, GPT is presented with a prompt tasking it to classify incoming inputs into one of three sentiment categories: \"positive\", \"neutral\", or \"negative\". This process occurs iteratively, with the prompt being included with each new request sent to the model. To enhance the objectivity and consistency of results, parameters such as temperature and seed are configured.\n",
    "\n",
    "The temperature parameter serves to temper the creativity of GPT's responses, promoting more objective outcomes. Meanwhile, the seed parameter is adjustable to any chosen value, ensuring greater consistency and determinism across multiple API calls, provided that the seed value remains constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17aac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the stream to OpenAI\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a249f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt for GPT\n",
    "initial_message = {\"role\": \"user\", \"content\": \"You are my sentiment assistant. I want you to analyze my text and categorize it into the three sentiments: positive, neutral, negative. Only respond with either positive, neutral or negative.\"}\n",
    "\n",
    "results=[]\n",
    "results_df = []\n",
    "count = 1\n",
    "\n",
    "# Sending labeling requests to GPT\n",
    "for index, eintrag in student_data.iterrows():\n",
    "    for inhalt in eintrag:\n",
    "        count=+1\n",
    "        display(count)\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                initial_message,\n",
    "                {\"role\": \"user\", \"content\": inhalt}\n",
    "            ],\n",
    "            #Setting temperature to ensure objective responses\n",
    "            temperature=0,\n",
    "            #Setting the seed parameter to ensure more consistent results\n",
    "            seed = 123\n",
    "        )\n",
    "        \n",
    "        # Saving of responses \n",
    "        results.append({\"index\": index, \"sentimentPrediction\": response.choices[0].message.content.lower()})\n",
    "\n",
    "# Conversion of responses into dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc10f0",
   "metadata": {},
   "source": [
    "# [1.1] First evaluation\n",
    "During the evaluation, a confusion matrix is employed to compare the predicted labels against the actual labels. It becomes apparent that GPT exhibits a bias towards categorizing inputs into more positive sentiments, evidenced by a precision of 1.0 for positive labels. Furthermore, the model achieves only an overall F1 score of 0.57. Notably, only 10 out of 132 negative datasets are correctly labeled, indicating a significant challenge in accurately identifying negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379e190",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Merge target labels with predicted labels and dropping of duplicated row\n",
    "results_df = results_df.drop(\"index\", axis=1)\n",
    "student_data = pd.concat([student_data_unpr, results_df], axis=1)\n",
    "student_data = student_data[student_data[\"Studium_Label\"].notnull()]\n",
    "\n",
    "\n",
    "#Convertion of all values into strings\n",
    "for x in student_data:\n",
    "    student_data[x] = student_data[x].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e1dfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Creation of confusion matrix\n",
    "confusion_mat = confusion_matrix(student_data['Studium_Label'], student_data['sentimentPrediction'], labels=['positive', 'negative', 'neutral'])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, display_labels=['positive', 'negative', 'neutral'])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "#Accuracy\n",
    "print(f1_score(student_data['Studium_Label'], student_data['sentimentPrediction'], labels=['positive', 'negative', 'neutral'], average='micro'))\n",
    "\n",
    "#Creation of presicion, recall, F1 and support\n",
    "print(classification_report(\n",
    "    student_data['Studium_Label'],\n",
    "    student_data['sentimentPrediction'],\n",
    "    output_dict=False,\n",
    "    target_names=['positive', 'negative', 'neutral']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd2771",
   "metadata": {},
   "source": [
    "# [2] Alteration of prompt towards more negative categorization\n",
    "As demonstrated in the evaluation, GPT 3.5 exhibited a bias towards categorizing inputs into more positive sentiments. To improve the accuracy and achieve a more negative categorization tendency, GPT was presented with an altered prompt designed to influence its categorization towards negativity.\n",
    "\n",
    "The same parameters, with temperature set to 0 and seed set to 123 as in the initial approach, were utilized for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e04e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt for GPT\n",
    "initial_message = {\"role\": \"user\", \"content\": \"You are my sentiment assistant. I want you to analyze my text and categorize it into the three sentiments: positive, neutral, negative. However, if a text has negative aspects, have a higher tendency towards a negative rating. Only respond with either positive, neutral or negative.\"}\n",
    "\n",
    "#Creation of a new data set\n",
    "student_data_altered = student_data_unpr\n",
    "student_data_altered = student_data_unpr.drop(\"Studium_Label\", axis=1)\n",
    "\n",
    "results=[]\n",
    "results_df = []\n",
    "\n",
    "# Sending labeling requests to GPT\n",
    "for index, eintrag in student_data_altered.iterrows():\n",
    "    for inhalt in eintrag:\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                initial_message,\n",
    "                {\"role\": \"user\", \"content\": inhalt}\n",
    "            ],\n",
    "            #Setting temperature to ensure objective responses\n",
    "            temperature=0,\n",
    "            #Setting the seed parameter to ensure more consistent results\n",
    "            seed = 123\n",
    "        )\n",
    "        \n",
    "        # Saving of responses \n",
    "        results.append({\"index\": index, \"alteredSentimentPrediction\": response.choices[0].message.content.lower()})\n",
    "\n",
    "# Conversion of responses into dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af0b83",
   "metadata": {},
   "source": [
    "# [2.1] Second evaluation \n",
    "In the second evaluation, GPT persists in demonstrating a bias towards positive categorization, with a reduced precision of 0.78 for positive sentiments, compared to the first evaluation. However, despite this bias, the overall F1 score shows an improvement, increasing by 4%. Nevertheless, GPT continues to struggle with accurately categorizing negative sentiments, correctly identifying only 27 out of 132 negatively labeled datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152c40f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Merging target labels with predicted labels and dropping of duplicated row\n",
    "results_df = results_df.drop(\"index\", axis=1)\n",
    "student_data_altered = pd.concat([student_data_unpr, results_df], axis=1)\n",
    "student_data_altered = student_data_altered[student_data_altered[\"Studium_Label\"].notnull()]\n",
    "\n",
    "#Adding of altered predictions to a dataframe in order to save all predictions in an excel\n",
    "altered_prediction = student_data_altered[\"alteredSentimentPrediction\"]\n",
    "student_data = pd.concat([student_data, altered_prediction], axis=1)\n",
    "\n",
    "\n",
    "#Convertion of all values into strings\n",
    "for x in student_data_altered:\n",
    "    student_data[x] = student_data[x].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98051ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving of data into an excel sheet\n",
    "#student_data.to_excel(\"Evaluation.xlsx\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b54fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Creation of confusion matrix\n",
    "confusion_mat = confusion_matrix(student_data_altered['Studium_Label'], student_data_altered['alteredSentimentPrediction'], labels=['positive', 'negative', 'neutral'])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, display_labels=['positive', 'negative', 'neutral'])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "#Accuracy\n",
    "print(f1_score(student_data_altered['Studium_Label'], student_data_altered['alteredSentimentPrediction'], labels=['positive', 'negative', 'neutral'], average='micro'))\n",
    "\n",
    "#Creation of presicion, recall, F1 and support\n",
    "print(classification_report(\n",
    "    student_data_altered['Studium_Label'],\n",
    "    student_data_altered['alteredSentimentPrediction'],\n",
    "    output_dict=False,\n",
    "    target_names=['positive', 'negative', 'neutral']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb65ccc",
   "metadata": {},
   "source": [
    "# [3] One shot learning with altered prompt\n",
    "As demonstrated in the evaluation, GPT 3.5 exhibited a bias towards categorizing inputs into more positive sentiments. To improve the accuracy and achieve a more negative categorization tendency, GPT was presented with an altered prompt designed to influence its categorization towards negativity.\n",
    "\n",
    "The same parameters, with temperature set to 0 and seed set to 123 as in the initial approach, were utilized for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee990d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt for GPT\n",
    "initial_message = {\"role\": \"user\", \"content\": \"You are my sentiment assistant. I want you to analyze my text and categorize it into the three sentiments: positive, neutral, negative. However, if a text has negative aspects, have a higher tendency towards a negative rating. This is an example for a negative labeled dataset: 'Die City University of Hong Kong hat eine große Kursauswahl. Die Kurse gehen für gewöhnlich drei Stunden lang und sie haben einen hohen Workload (vor allem im letzten Drittel der Vorlesungszeit). Das Kursniveau generell nicht auf dem von Mannheim. Es könnte ein kultureller Unterschied sein, aber die Fragestellungen für Essays und Gruppenarbeiten sind oftmals sehr vage und breit gefasst. Auch die Bewertung ist intransparent, man erhält kein Feedback bevor man die Endnote für den Kurs erhält. Mein Semester ging von Ende August bis Ende November (Vorlesungszeit). Nach einer Woche Pause beginnt die zwei wöchige Prüfungszeit und endet vor Weihnachten. Ich hatte jedoch keine Klausur in der Prüfungsphase, alle Abgaben (Essays, Gruppenberichte) und Präsentationen/Tests liefen unter dem Semester.Die Mensen der CityU sind sehr empfehlenswert. Westliches und asiatisches Essen ist beides vorhanden. Vor allem die Desserts in AC1 sind eine große Empfehlung.Das Sprachniveau war sehr durchmischt, sowohl bei Professoren, als auch bei Studenten. Teils waren Fragestellungen in Prüfungen sprachlich schwer verständlich. In Restaurants kommt man gut damit aus auf die Bilder/Karte zu zeigen.' This is an example for a neutral dataset: 'Da ich während Corona vor Ort war habe ich die Uni nur zweimal besucht. Die restlichen Monate hatte ich nur Online Vorlesungen. Von den Professoren und den Mitarbeitern kann ich sagen, dasss alle sehr nett und hilfsbereit waren. Falls ihr Probleme oder Fragen habt sendet einfach eine Email an das Studienbüro und die werden euch helfen.' Only respond with either positive, neutral or negative.\"}\n",
    "\n",
    "#Creation of a new data set\n",
    "student_data_oneshot = student_data_unpr\n",
    "student_data_oneshot = student_data_unpr.drop(\"Studium_Label\", axis=1)\n",
    "\n",
    "results=[]\n",
    "results_df = []\n",
    "\n",
    "# Sending labeling requests to GPT\n",
    "for index, eintrag in student_data_oneshot.iterrows():\n",
    "    for inhalt in eintrag:\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                initial_message,\n",
    "                {\"role\": \"user\", \"content\": inhalt}\n",
    "            ],\n",
    "            #Setting temperature to ensure objective responses\n",
    "            temperature=0,\n",
    "            #Setting the seed parameter to ensure more consistent results\n",
    "            seed = 123\n",
    "        )\n",
    "        \n",
    "        # Saving of responses \n",
    "        results.append({\"index\": index, \"oneshotSentimentPrediction\": response.choices[0].message.content.lower()})\n",
    "\n",
    "# Conversion of responses into dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf85ba1a",
   "metadata": {},
   "source": [
    "# [3.1] One shot evaluation \n",
    "In the second evaluation, GPT persists in demonstrating a bias towards positive categorization, with a reduced precision of 0.78 for positive sentiments, compared to the first evaluation. However, despite this bias, the overall F1 score shows an improvement, increasing by 4%. Nevertheless, GPT continues to struggle with accurately categorizing negative sentiments, correctly identifying only 27 out of 132 negatively labeled datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a877f855",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Merging target labels with predicted labels and dropping of duplicated row\n",
    "results_df = results_df.drop(\"index\", axis=1)\n",
    "student_data_oneshot = pd.concat([student_data_unpr, results_df], axis=1)\n",
    "student_data_oneshot = student_data_oneshot[student_data_oneshot[\"Studium_Label\"].notnull()]\n",
    "\n",
    "#Adding of altered predictions to a dataframe in order to save all predictions in an excel\n",
    "oneshot_prediction = student_data_oneshot[\"oneshotSentimentPrediction\"]\n",
    "student_data = pd.concat([student_data, altered_oneshot], axis=1)\n",
    "\n",
    "\n",
    "#Convertion of all values into strings\n",
    "for x in student_data_oneshot:\n",
    "    student_data[x] = student_data[x].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving of data into an excel sheet\n",
    "student_data.to_excel(\"Evaluation.xlsx\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd7647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Creation of confusion matrix\n",
    "confusion_mat = confusion_matrix(student_data_oneshot['Studium_Label'], student_data_oneshot['oneshotSentimentPrediction'], labels=['positive', 'negative', 'neutral'])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, display_labels=['positive', 'negative', 'neutral'])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "#Accuracy\n",
    "print(f1_score(student_data_oneshot['Studium_Label'], student_data_oneshot['oneshotSentimentPrediction'], labels=['positive', 'negative', 'neutral'], average='micro'))\n",
    "\n",
    "#Creation of presicion, recall, F1 and support\n",
    "print(classification_report(\n",
    "    student_data_oneshot['Studium_Label'],\n",
    "    student_data_oneshot['oneshotSentimentPrediction'],\n",
    "    output_dict=False,\n",
    "    target_names=['positive', 'negative', 'neutral']\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
